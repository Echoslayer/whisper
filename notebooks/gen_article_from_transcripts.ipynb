{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Article from Transcripts\n",
    "\n",
    "This notebook allows you to process a transcript file and convert its conversational content into a well-structured, readable article format using an AI API (like OpenAI or Grok). You can configure the settings using interactive widgets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Configuration\n",
    "\n",
    "Configure the input transcript file, output directory, API key, and model settings using interactive widgets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bbba52370a348a1ac6522610efef434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='../data/demo/clean_transcription.txt', description='Input File:', layout=Layout(width='500px'), plâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6b884e378ef42ee96d2168aae10d207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='../data/articles', description='Output Dir:', layout=Layout(width='500px'), placeholder='Enter outâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79b09169dd84f5a9438febf97f62292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='article.txt', description='Output File:', layout=Layout(width='500px'), placeholder='Enter output â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d916d722698e4517a53f13e0e72d67f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='grok-3-mini-fast-latest', description='Model:', layout=Layout(width='500px'), placeholder='Enter mâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3be5e08ce8504283ae8590c8ed3e6cee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Provider:', index=1, layout=Layout(width='500px'), options=(('OpenAI', 'openai'), ('Grokâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key is loaded from environment variables.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(dotenv_path=\"../.env\")\n",
    "\n",
    "# Default configuration values\n",
    "DEFAULT_INPUT_FILE = \"../data/demo/clean_transcription.txt\"\n",
    "DEFAULT_OUTPUT_DIR = \"../data/articles\"\n",
    "DEFAULT_OUTPUT_FILE = \"article.txt\"\n",
    "DEFAULT_MODEL = \"grok-3-mini-fast-latest\"\n",
    "DEFAULT_PROVIDER = \"grok\"\n",
    "\n",
    "# Widgets for configuration\n",
    "input_file_widget = widgets.Text(\n",
    "    value=DEFAULT_INPUT_FILE,\n",
    "    placeholder='Enter input transcript file path',\n",
    "    description='Input File:',\n",
    "    layout={'width': '500px'}\n",
    ")\n",
    "\n",
    "output_dir_widget = widgets.Text(\n",
    "    value=DEFAULT_OUTPUT_DIR,\n",
    "    placeholder='Enter output directory',\n",
    "    description='Output Dir:',\n",
    "    layout={'width': '500px'}\n",
    ")\n",
    "\n",
    "output_file_widget = widgets.Text(\n",
    "    value=DEFAULT_OUTPUT_FILE,\n",
    "    placeholder='Enter output article filename',\n",
    "    description='Output File:',\n",
    "    layout={'width': '500px'}\n",
    ")\n",
    "\n",
    "model_widget = widgets.Text(\n",
    "    value=DEFAULT_MODEL,\n",
    "    placeholder='Enter model name',\n",
    "    description='Model:',\n",
    "    layout={'width': '500px'}\n",
    ")\n",
    "\n",
    "provider_widget = widgets.Dropdown(\n",
    "    options=[('OpenAI', 'openai'), ('Grok (via xAI)', 'grok'), ('Other', 'other')],\n",
    "    value=DEFAULT_PROVIDER,\n",
    "    description='Provider:',\n",
    "    layout={'width': '500px'}\n",
    ")\n",
    "\n",
    "# Display widgets\n",
    "display(input_file_widget)\n",
    "display(output_dir_widget)\n",
    "display(output_file_widget)\n",
    "display(model_widget)\n",
    "display(provider_widget)\n",
    "print(\"API key is loaded from environment variables.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Article Generation Function\n",
    "\n",
    "Define a function to process the transcript and generate a readable article using the AI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import completion\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def generate_article(transcript_text, api_key, model, provider):\n",
    "    \"\"\"Generate a readable article from transcript text using an AI API.\n",
    "    \n",
    "    Args:\n",
    "        transcript_text (str): The raw transcript text.\n",
    "        api_key (str): API key for the AI service.\n",
    "        model (str): Model name to use for generation.\n",
    "        provider (str): AI service provider ('openai', 'grok', etc.).\n",
    "    \n",
    "    Returns:\n",
    "        str: The generated article text.\n",
    "    \"\"\"\n",
    "    # Map provider to litellm model prefix if needed\n",
    "    if provider == 'openai':\n",
    "        model_name = model\n",
    "        api_base = None\n",
    "    elif provider == 'grok':\n",
    "        model_name = f\"xai/{model}\" if not model.startswith(\"xai/\") else model\n",
    "        api_base = None  # litellm handles the API base for xAI\n",
    "    else:\n",
    "        model_name = model\n",
    "        api_base = None\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are an expert writer tasked with converting a conversational transcript into a well-structured, readable article.\n",
    "    The transcript may contain colloquial language, repetitions, and filler words. Your goal is to:\n",
    "    1. Remove unnecessary repetitions and filler content.\n",
    "    2. Convert spoken language into formal, written language.\n",
    "    3. Preserve the original meaning and key points of the content.\n",
    "    4. Organize the content into logical paragraphs with a clear flow.\n",
    "    5. Use headings or subheadings if appropriate to structure the article.\n",
    "    \n",
    "    Here is the transcript to process:\n",
    "    \n",
    "    {transcript_text}\n",
    "    \n",
    "    Output the polished article below:\n",
    "    \"\"\"\n",
    "    \n",
    "    response = completion(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a professional writer.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        api_key=api_key,\n",
    "        api_base=api_base,\n",
    "        temperature=0.7,\n",
    "        max_tokens=2000\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def save_article(article_text, output_path):\n",
    "    \"\"\"Save the generated article to a file.\n",
    "    \n",
    "    Args:\n",
    "        article_text (str): The article content to save.\n",
    "        output_path (str): Path to save the article file.\n",
    "    \"\"\"\n",
    "    Path(os.path.dirname(output_path)).mkdir(parents=True, exist_ok=True)\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(article_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generate Article from Transcript\n",
    "\n",
    "Read the transcript file and use the AI API to generate a polished article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“– è®€å–é€å­—ç¨¿æª”æ¡ˆï¼š../data/demo/clean_transcription.txt\n",
      "âœï¸ ä½¿ç”¨ AI ç”Ÿæˆæ–‡ç« ä¸­...\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "âŒ è™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼šlitellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=grok/grok-3-mini-fast-latest\n",
      " Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Get values from widgets\n",
    "    input_file = input_file_widget.value\n",
    "    output_dir = output_dir_widget.value\n",
    "    output_file = output_file_widget.value\n",
    "    model = model_widget.value\n",
    "    provider = provider_widget.value\n",
    "    \n",
    "    # Load API key from environment variables\n",
    "    api_key = os.getenv('API_KEY', '')\n",
    "    if not api_key:\n",
    "        raise ValueError(f\"âŒ è«‹åœ¨ .env æª”æ¡ˆä¸­æä¾› API é‡‘é‘°\")\n",
    "    \n",
    "    output_path = os.path.join(output_dir, output_file)\n",
    "    \n",
    "    # Check if input file exists\n",
    "    if not os.path.exists(input_file):\n",
    "        raise FileNotFoundError(f\"âŒ æ‰¾ä¸åˆ°è¼¸å…¥é€å­—ç¨¿æª”æ¡ˆï¼š{input_file}\")\n",
    "    \n",
    "    # Read the transcript file\n",
    "    print(f\"ğŸ“– è®€å–é€å­—ç¨¿æª”æ¡ˆï¼š{input_file}\")\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        transcript_text = f.read()\n",
    "    \n",
    "    # Generate the article\n",
    "    print(\"âœï¸ ä½¿ç”¨ AI ç”Ÿæˆæ–‡ç« ä¸­...\")\n",
    "    article_text = generate_article(transcript_text, api_key, model, provider)\n",
    "    \n",
    "    # Save the article\n",
    "    save_article(article_text, output_path)\n",
    "    print(f\"ğŸ‰ æ–‡ç« ç”Ÿæˆå®Œæˆï¼çµæœå·²å„²å­˜è‡³ {output_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ è™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼š{e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
