{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Transcript from Audio\n",
    "\n",
    "This notebook allows you to process an audio file, split it into clips, and transcribe the content using Whisper.cpp. You can configure the settings using interactive widgets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Configuration\n",
    "\n",
    "Configure the input file, output directory, and other settings using interactive widgets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "646df4a6e9ef41f28c1d7361a42afd28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='../data//demo/demo.wav', description='Input File:', layout=Layout(width='500px'), placeholder='Entâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ca89d0be791403aa44bca0ae1152fa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='../data/output_clips', description='Output Dir:', layout=Layout(width='500px'), placeholder='Enterâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b36bd9191974c08b642533c5ec77163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=1, description='Clip Duration (min):', layout=Layout(width='500px'), max=30, min=1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "729512a79b3e4825ac7a898b9892d788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='../whisper.cpp/build/bin/whisper-cli', description='Whisper Exec:', layout=Layout(width='500px'), â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10eb81eabba6419883f8c3aa9bc1471f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='../whisper.cpp/models/ggml-medium.bin', description='Whisper Model:', layout=Layout(width='500px')â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "048ba1caadb4430dbd7ccda516aa299e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Language:', layout=Layout(width='500px'), options=(('Chinese (zh)', 'zh'), ('English (enâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f87b872e69074162bcdf78fe0726c26e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Transcript File:', layout=Layout(width='500px'), placeholder='Enter transcript filâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Default configuration values\n",
    "DEFAULT_INPUT_FILE = \"../data//demo/demo.wav\"\n",
    "# DEFAULT_INPUT_FILE = \"/Volumes/MacHD/æ¢µå…¬å­/æ¢µå…¬å­ã€ç¾å¥³èµ„æºæ”¶é›†å™¨ã€‘/ç¬¬å…­èŠ‚ã€è¯†åˆ«â€œSâ€å¥³ã€‘å¿…å¬/ç¬¬å…­èŠ‚ã€è¯†åˆ«â€œsâ€å¥³ã€‘ã€WWW.PUACP.COMã€‘.mp4\"\n",
    "DEFAULT_OUTPUT_DIR = \"../data/output_clips\"\n",
    "DEFAULT_CLIP_DURATION_MIN = 1  # in minutes\n",
    "DEFAULT_WHISPER_EXEC = \"../whisper.cpp/build/bin/whisper-cli\"\n",
    "DEFAULT_WHISPER_MODEL = \"../whisper.cpp/models/ggml-medium.bin\"\n",
    "DEFAULT_LANGUAGE = \"zh\"\n",
    "DEFAULT_TRANSCRIPT_FILENAME = \"transcription.txt\"\n",
    "\n",
    "# Widgets for configuration\n",
    "input_file_widget = widgets.Text(\n",
    "    value=DEFAULT_INPUT_FILE,\n",
    "    placeholder='Enter input audio file path',\n",
    "    description='Input File:',\n",
    "    layout={'width': '500px'}\n",
    ")\n",
    "\n",
    "output_dir_widget = widgets.Text(\n",
    "    value=DEFAULT_OUTPUT_DIR,\n",
    "    placeholder='Enter output directory',\n",
    "    description='Output Dir:',\n",
    "    layout={'width': '500px'}\n",
    ")\n",
    "\n",
    "clip_duration_widget = widgets.IntSlider(\n",
    "    value=DEFAULT_CLIP_DURATION_MIN,\n",
    "    min=1,\n",
    "    max=30,\n",
    "    step=1,\n",
    "    description='Clip Duration (min):',\n",
    "    layout={'width': '500px'}\n",
    ")\n",
    "\n",
    "whisper_exec_widget = widgets.Text(\n",
    "    value=DEFAULT_WHISPER_EXEC,\n",
    "    placeholder='Enter Whisper.cpp executable path',\n",
    "    description='Whisper Exec:',\n",
    "    layout={'width': '500px'}\n",
    ")\n",
    "\n",
    "whisper_model_widget = widgets.Text(\n",
    "    value=DEFAULT_WHISPER_MODEL,\n",
    "    placeholder='Enter Whisper model path',\n",
    "    description='Whisper Model:',\n",
    "    layout={'width': '500px'}\n",
    ")\n",
    "\n",
    "language_widget = widgets.Dropdown(\n",
    "    options=[('Chinese (zh)', 'zh'), ('English (en)', 'en')],\n",
    "    value=DEFAULT_LANGUAGE,\n",
    "    description='Language:',\n",
    "    layout={'width': '500px'}\n",
    ")\n",
    "\n",
    "transcript_filename_widget = widgets.Text(\n",
    "    value='',\n",
    "    placeholder=f'Enter transcript filename (default: {DEFAULT_TRANSCRIPT_FILENAME})',\n",
    "    description='Transcript File:',\n",
    "    layout={'width': '500px'}\n",
    ")\n",
    "\n",
    "# Display widgets\n",
    "display(input_file_widget)\n",
    "display(output_dir_widget)\n",
    "display(clip_duration_widget)\n",
    "display(whisper_exec_widget)\n",
    "display(whisper_model_widget)\n",
    "display(language_widget)\n",
    "display(transcript_filename_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Audio Processing Functions\n",
    "\n",
    "Import the necessary functions for processing audio files from `voice2transcripts.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the scripts directory to the path so we can import the functions\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../scripts')))\n",
    "from voice2transcripts import clear_output_folder, convert_to_wav, split_audio, transcribe_audio\n",
    "from time_stamp_cleaner import clean_transcription, save_cleaned_transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Process Audio and Generate Transcript\n",
    "\n",
    "Run the processing pipeline to convert, split, transcribe, and clean the audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ—‘ï¸ è¼¸å‡ºè³‡æ–™å¤¾å·²æ¸…é™¤ï¼\n",
      "ğŸš€ é–‹å§‹éŸ³è¨Šè™•ç†èˆ‡è½‰éŒ„æµç¨‹...\n",
      "ğŸ”„ éŸ³è¨Šå·²è½‰æ›ç‚º WAV æ ¼å¼ï¼š../data/output_clips/converted.wav\n",
      "âœ… éŸ³è¨Šåˆ‡å‰²å®Œæˆï¼Œå…± 2 å€‹ç‰‡æ®µï¼Œæ™‚é–“æˆ³å·²å„²å­˜\n",
      "ğŸ¤ è½‰éŒ„ç‰‡æ®µ 1/2: clip_001.wav ...\n",
      "âš ï¸ Whisper.cpp è¨Šæ¯: error: unknown argument: --timestamps\n",
      "\n",
      "usage: ../whisper.cpp/build/bin/whisper-cli [options] file0 file1 ...\n",
      "supported audio formats: flac, mp3, ogg, wav\n",
      "\n",
      "options:\n",
      "  -h,        --help              [default] show this help message and exit\n",
      "  -t N,      --threads N         [4      ] number of threads to use during computation\n",
      "  -p N,      --processors N      [1      ] number of processors to use during computation\n",
      "  -ot N,     --offset-t N        [0      ] time offset in milliseconds\n",
      "  -on N,     --offset-n N        [0      ] segment index offset\n",
      "  -d  N,     --duration N        [0      ] duration of audio to process in milliseconds\n",
      "  -mc N,     --max-context N     [-1     ] maximum number of text context tokens to store\n",
      "  -ml N,     --max-len N         [0      ] maximum segment length in characters\n",
      "  -sow,      --split-on-word     [false  ] split on word rather than on token\n",
      "  -bo N,     --best-of N         [5      ] number of best candidates to keep\n",
      "  -bs N,     --beam-size N       [5      ] beam size for beam search\n",
      "  -ac N,     --audio-ctx N       [0      ] audio context size (0 - all)\n",
      "  -wt N,     --word-thold N      [0.01   ] word timestamp probability threshold\n",
      "  -et N,     --entropy-thold N   [2.40   ] entropy threshold for decoder fail\n",
      "  -lpt N,    --logprob-thold N   [-1.00  ] log probability threshold for decoder fail\n",
      "  -nth N,    --no-speech-thold N [0.60   ] no speech threshold\n",
      "  -tp,       --temperature N     [0.00   ] The sampling temperature, between 0 and 1\n",
      "  -tpi,      --temperature-inc N [0.20   ] The increment of temperature, between 0 and 1\n",
      "  -debug,    --debug-mode        [false  ] enable debug mode (eg. dump log_mel)\n",
      "  -tr,       --translate         [false  ] translate from source language to english\n",
      "  -di,       --diarize           [false  ] stereo audio diarization\n",
      "  -tdrz,     --tinydiarize       [false  ] enable tinydiarize (requires a tdrz model)\n",
      "  -nf,       --no-fallback       [false  ] do not use temperature fallback while decoding\n",
      "  -otxt,     --output-txt        [false  ] output result in a text file\n",
      "  -ovtt,     --output-vtt        [false  ] output result in a vtt file\n",
      "  -osrt,     --output-srt        [false  ] output result in a srt file\n",
      "  -olrc,     --output-lrc        [false  ] output result in a lrc file\n",
      "  -owts,     --output-words      [false  ] output script for generating karaoke video\n",
      "  -fp,       --font-path         [/System/Library/Fonts/Supplemental/Courier New Bold.ttf] path to a monospace font for karaoke video\n",
      "  -ocsv,     --output-csv        [false  ] output result in a CSV file\n",
      "  -oj,       --output-json       [false  ] output result in a JSON file\n",
      "  -ojf,      --output-json-full  [false  ] include more information in the JSON file\n",
      "  -of FNAME, --output-file FNAME [       ] output file path (without file extension)\n",
      "  -np,       --no-prints         [false  ] do not print anything other than the results\n",
      "  -ps,       --print-special     [false  ] print special tokens\n",
      "  -pc,       --print-colors      [false  ] print colors\n",
      "             --print-confidence  [false  ] print confidence\n",
      "  -pp,       --print-progress    [false  ] print progress\n",
      "  -nt,       --no-timestamps     [false  ] do not print timestamps\n",
      "  -l LANG,   --language LANG     [zh     ] spoken language ('auto' for auto-detect)\n",
      "  -dl,       --detect-language   [false  ] exit after automatically detecting language\n",
      "             --prompt PROMPT     [       ] initial prompt (max n_text_ctx/2 tokens)\n",
      "  -m FNAME,  --model FNAME       [../whisper.cpp/models/ggml-medium.bin] model path\n",
      "  -f FNAME,  --file FNAME        [       ] input audio file path\n",
      "  -oved D,   --ov-e-device DNAME [CPU    ] the OpenVINO device used for encode inference\n",
      "  -dtw MODEL --dtw MODEL         [       ] compute token-level timestamps\n",
      "  -ls,       --log-score         [false  ] log best decoder scores of tokens\n",
      "  -ng,       --no-gpu            [false  ] disable GPU\n",
      "  -fa,       --flash-attn        [false  ] flash attention\n",
      "  -sns,      --suppress-nst      [false  ] suppress non-speech tokens\n",
      "  --suppress-regex REGEX         [       ] regular expression matching tokens to suppress\n",
      "  --grammar GRAMMAR              [       ] GBNF grammar to guide decoding\n",
      "  --grammar-rule RULE            [       ] top-level GBNF grammar rule name\n",
      "  --grammar-penalty N            [100.0  ] scales down logits of nongrammar tokens\n",
      "\n",
      "Voice Activity Detection (VAD) options:\n",
      "             --vad                           [false  ] enable Voice Activity Detection (VAD)\n",
      "  -vm FNAME, --vad-model FNAME               [       ] VAD model path\n",
      "  -vt N,     --vad-threshold N               [0.50   ] VAD threshold for speech recognition\n",
      "  -vspd N,   --vad-min-speech-duration-ms  N [250    ] VAD min speech duration (0.0-1.0)\n",
      "  -vsd N,    --vad-min-silence-duration-ms N [100    ] VAD min silence duration (to split segments)\n",
      "  -vmsd N,   --vad-max-speech-duration-s   N [FLT_MAX] VAD max speech duration (auto-split longer)\n",
      "  -vp N,     --vad-speech-pad-ms           N [30     ] VAD speech padding (extend segments)\n",
      "  -vo N,     --vad-samples-overlap         N [0.10   ] VAD samples overlap (seconds between segments)\n",
      "âœ… ç‰‡æ®µ 1/2 è½‰éŒ„å®Œæˆ\n",
      "ğŸ¤ è½‰éŒ„ç‰‡æ®µ 2/2: clip_002.wav ...\n",
      "âš ï¸ Whisper.cpp è¨Šæ¯: error: unknown argument: --timestamps\n",
      "\n",
      "usage: ../whisper.cpp/build/bin/whisper-cli [options] file0 file1 ...\n",
      "supported audio formats: flac, mp3, ogg, wav\n",
      "\n",
      "options:\n",
      "  -h,        --help              [default] show this help message and exit\n",
      "  -t N,      --threads N         [4      ] number of threads to use during computation\n",
      "  -p N,      --processors N      [1      ] number of processors to use during computation\n",
      "  -ot N,     --offset-t N        [0      ] time offset in milliseconds\n",
      "  -on N,     --offset-n N        [0      ] segment index offset\n",
      "  -d  N,     --duration N        [0      ] duration of audio to process in milliseconds\n",
      "  -mc N,     --max-context N     [-1     ] maximum number of text context tokens to store\n",
      "  -ml N,     --max-len N         [0      ] maximum segment length in characters\n",
      "  -sow,      --split-on-word     [false  ] split on word rather than on token\n",
      "  -bo N,     --best-of N         [5      ] number of best candidates to keep\n",
      "  -bs N,     --beam-size N       [5      ] beam size for beam search\n",
      "  -ac N,     --audio-ctx N       [0      ] audio context size (0 - all)\n",
      "  -wt N,     --word-thold N      [0.01   ] word timestamp probability threshold\n",
      "  -et N,     --entropy-thold N   [2.40   ] entropy threshold for decoder fail\n",
      "  -lpt N,    --logprob-thold N   [-1.00  ] log probability threshold for decoder fail\n",
      "  -nth N,    --no-speech-thold N [0.60   ] no speech threshold\n",
      "  -tp,       --temperature N     [0.00   ] The sampling temperature, between 0 and 1\n",
      "  -tpi,      --temperature-inc N [0.20   ] The increment of temperature, between 0 and 1\n",
      "  -debug,    --debug-mode        [false  ] enable debug mode (eg. dump log_mel)\n",
      "  -tr,       --translate         [false  ] translate from source language to english\n",
      "  -di,       --diarize           [false  ] stereo audio diarization\n",
      "  -tdrz,     --tinydiarize       [false  ] enable tinydiarize (requires a tdrz model)\n",
      "  -nf,       --no-fallback       [false  ] do not use temperature fallback while decoding\n",
      "  -otxt,     --output-txt        [false  ] output result in a text file\n",
      "  -ovtt,     --output-vtt        [false  ] output result in a vtt file\n",
      "  -osrt,     --output-srt        [false  ] output result in a srt file\n",
      "  -olrc,     --output-lrc        [false  ] output result in a lrc file\n",
      "  -owts,     --output-words      [false  ] output script for generating karaoke video\n",
      "  -fp,       --font-path         [/System/Library/Fonts/Supplemental/Courier New Bold.ttf] path to a monospace font for karaoke video\n",
      "  -ocsv,     --output-csv        [false  ] output result in a CSV file\n",
      "  -oj,       --output-json       [false  ] output result in a JSON file\n",
      "  -ojf,      --output-json-full  [false  ] include more information in the JSON file\n",
      "  -of FNAME, --output-file FNAME [       ] output file path (without file extension)\n",
      "  -np,       --no-prints         [false  ] do not print anything other than the results\n",
      "  -ps,       --print-special     [false  ] print special tokens\n",
      "  -pc,       --print-colors      [false  ] print colors\n",
      "             --print-confidence  [false  ] print confidence\n",
      "  -pp,       --print-progress    [false  ] print progress\n",
      "  -nt,       --no-timestamps     [false  ] do not print timestamps\n",
      "  -l LANG,   --language LANG     [zh     ] spoken language ('auto' for auto-detect)\n",
      "  -dl,       --detect-language   [false  ] exit after automatically detecting language\n",
      "             --prompt PROMPT     [       ] initial prompt (max n_text_ctx/2 tokens)\n",
      "  -m FNAME,  --model FNAME       [../whisper.cpp/models/ggml-medium.bin] model path\n",
      "  -f FNAME,  --file FNAME        [       ] input audio file path\n",
      "  -oved D,   --ov-e-device DNAME [CPU    ] the OpenVINO device used for encode inference\n",
      "  -dtw MODEL --dtw MODEL         [       ] compute token-level timestamps\n",
      "  -ls,       --log-score         [false  ] log best decoder scores of tokens\n",
      "  -ng,       --no-gpu            [false  ] disable GPU\n",
      "  -fa,       --flash-attn        [false  ] flash attention\n",
      "  -sns,      --suppress-nst      [false  ] suppress non-speech tokens\n",
      "  --suppress-regex REGEX         [       ] regular expression matching tokens to suppress\n",
      "  --grammar GRAMMAR              [       ] GBNF grammar to guide decoding\n",
      "  --grammar-rule RULE            [       ] top-level GBNF grammar rule name\n",
      "  --grammar-penalty N            [100.0  ] scales down logits of nongrammar tokens\n",
      "\n",
      "Voice Activity Detection (VAD) options:\n",
      "             --vad                           [false  ] enable Voice Activity Detection (VAD)\n",
      "  -vm FNAME, --vad-model FNAME               [       ] VAD model path\n",
      "  -vt N,     --vad-threshold N               [0.50   ] VAD threshold for speech recognition\n",
      "  -vspd N,   --vad-min-speech-duration-ms  N [250    ] VAD min speech duration (0.0-1.0)\n",
      "  -vsd N,    --vad-min-silence-duration-ms N [100    ] VAD min silence duration (to split segments)\n",
      "  -vmsd N,   --vad-max-speech-duration-s   N [FLT_MAX] VAD max speech duration (auto-split longer)\n",
      "  -vp N,     --vad-speech-pad-ms           N [30     ] VAD speech padding (extend segments)\n",
      "  -vo N,     --vad-samples-overlap         N [0.10   ] VAD samples overlap (seconds between segments)\n",
      "âœ… ç‰‡æ®µ 2/2 è½‰éŒ„å®Œæˆ\n",
      "ğŸ‰ è½‰éŒ„è™•ç†å®Œæˆï¼è½‰éŒ„çµæœå·²å„²å­˜è‡³ ../data/output_clips/../transcripts/transcription.txt\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Get values from widgets\n",
    "    input_file = input_file_widget.value\n",
    "    output_dir = output_dir_widget.value\n",
    "    clip_duration_sec = clip_duration_widget.value * 60  # Convert minutes to seconds\n",
    "    whisper_exec = whisper_exec_widget.value\n",
    "    whisper_model = whisper_model_widget.value\n",
    "    language = language_widget.value\n",
    "    transcript_filename = transcript_filename_widget.value if transcript_filename_widget.value.strip() else DEFAULT_TRANSCRIPT_FILENAME\n",
    "\n",
    "    # Check if input file exists\n",
    "    if not os.path.exists(input_file):\n",
    "        raise FileNotFoundError(f\"âŒ æ‰¾ä¸åˆ°è¼¸å…¥éŸ³è¨Šæª”æ¡ˆï¼š{input_file}\")\n",
    "\n",
    "    # Clear old files\n",
    "    clear_output_folder(output_dir)\n",
    "\n",
    "    # Execute the processing pipeline\n",
    "    print(\"ğŸš€ é–‹å§‹éŸ³è¨Šè™•ç†èˆ‡è½‰éŒ„æµç¨‹...\")\n",
    "    wav_file = convert_to_wav(input_file, output_dir)\n",
    "    clip_files = split_audio(wav_file, clip_duration_sec, output_dir)\n",
    "    transcribe_audio(clip_files, output_dir, whisper_exec, whisper_model, language, transcript_filename)\n",
    "    print(f\"ğŸ‰ è½‰éŒ„è™•ç†å®Œæˆï¼è½‰éŒ„çµæœå·²å„²å­˜è‡³ {os.path.join(output_dir, '../transcripts/' + transcript_filename)}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ è™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼š{e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Clean Transcription\n",
    "\n",
    "Clean the transcription by removing per-sentence timestamps and formatting the content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.1: Configure Cleaned Transcript Filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d0302d336f042cc847a0bc6ed2252a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Cleaned Transcript:', layout=Layout(width='500px'), placeholder='Enter cleaned traâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Widget for cleaned transcript filename\n",
    "cleaned_transcript_filename_widget = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Enter cleaned transcript filename (default: clean_transcription.txt)',\n",
    "    description='Cleaned Transcript:',\n",
    "    layout={'width': '500px'}\n",
    ")\n",
    "display(cleaned_transcript_filename_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.2: Clean and Save Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§¹ é–‹å§‹æ¸…ç†è½‰éŒ„çµæœ...\n",
      "å˜—è©¦è®€å–è½‰éŒ„æª”æ¡ˆï¼š../data/transcripts/transcription.txt\n",
      "âœ… æ‰¾åˆ°è½‰éŒ„æª”æ¡ˆï¼Œé–‹å§‹æ¸…ç†...\n",
      "âš ï¸ æ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„è½‰éŒ„å…§å®¹ï¼Œç„¡æ³•å„²å­˜æ¸…ç†å¾Œçš„æª”æ¡ˆã€‚\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Get the transcript file path based on the output directory structure\n",
    "    transcript_dir = os.path.join(os.path.dirname(output_dir), 'transcripts')\n",
    "    transcript_path = os.path.join(transcript_dir, transcript_filename)\n",
    "    \n",
    "    # Get the cleaned transcript filename from the widget\n",
    "    cleaned_transcript_filename = cleaned_transcript_filename_widget.value if cleaned_transcript_filename_widget.value.strip() else \"clean_transcription.txt\"\n",
    "    cleaned_transcript_path = os.path.join(transcript_dir, cleaned_transcript_filename)\n",
    "    \n",
    "    # Read, clean, and save the transcription\n",
    "    print(\"ğŸ§¹ é–‹å§‹æ¸…ç†è½‰éŒ„çµæœ...\")\n",
    "    print(f\"å˜—è©¦è®€å–è½‰éŒ„æª”æ¡ˆï¼š{transcript_path}\")\n",
    "    if os.path.exists(transcript_path):\n",
    "        print(f\"âœ… æ‰¾åˆ°è½‰éŒ„æª”æ¡ˆï¼Œé–‹å§‹æ¸…ç†...\")\n",
    "        with open(transcript_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        global cleaned_segments\n",
    "        cleaned_segments = clean_transcription(text)\n",
    "        if cleaned_segments:\n",
    "            save_cleaned_transcription(cleaned_segments, cleaned_transcript_path)\n",
    "            print(f\"ğŸ‰ æ¸…ç†å®Œæˆï¼æ¸…ç†å¾Œçš„è½‰éŒ„çµæœå·²å„²å­˜è‡³ {cleaned_transcript_path}\")\n",
    "        else:\n",
    "            print(\"âš ï¸ æ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„è½‰éŒ„å…§å®¹ï¼Œç„¡æ³•å„²å­˜æ¸…ç†å¾Œçš„æª”æ¡ˆã€‚\")\n",
    "    else:\n",
    "        print(f\"âŒ è½‰éŒ„æª”æ¡ˆä¸å­˜åœ¨ï¼š{transcript_path}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"âŒ æ‰¾ä¸åˆ°æª”æ¡ˆï¼š{e}\")\n",
    "except IOError as e:\n",
    "    print(f\"âŒ è®€å–æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ æ¸…ç†è½‰éŒ„éç¨‹ä¸­ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤ï¼š{e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Convert Transcription to SRT Format\n",
    "\n",
    "Convert the cleaned transcription to SRT subtitle format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.1: Configure SRT Filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f759a214c9e7423482e8233f6970391f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='subtitles.srt', description='SRT File:', layout=Layout(width='500px'), placeholder='Enter SRT fileâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Widget for SRT filename\n",
    "srt_filename_widget = widgets.Text(\n",
    "    value='subtitles.srt',\n",
    "    placeholder='Enter SRT filename (default: subtitles.srt)',\n",
    "    description='SRT File:',\n",
    "    layout={'width': '500px'}\n",
    ")\n",
    "display(srt_filename_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.2: Generate SRT File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ é–‹å§‹è½‰æ›ç‚º SRT æ ¼å¼...\n",
      "âš ï¸ æ²’æœ‰æ¸…ç†å¾Œçš„è½‰éŒ„å…§å®¹å¯ä¾›è½‰æ›ç‚º SRT æ ¼å¼ã€‚\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from time_stamp_cleaner import convert_to_srt\n",
    "    \n",
    "    # Get the SRT filename from the widget\n",
    "    srt_filename = srt_filename_widget.value if srt_filename_widget.value.strip() else \"subtitles.srt\"\n",
    "    srt_path = os.path.join(transcript_dir, srt_filename)\n",
    "    \n",
    "    # Convert cleaned segments to SRT format\n",
    "    print(\"ğŸ“ é–‹å§‹è½‰æ›ç‚º SRT æ ¼å¼...\")\n",
    "    if cleaned_segments:\n",
    "        convert_to_srt(cleaned_segments, srt_path)\n",
    "        print(f\"ğŸ‰ è½‰æ›å®Œæˆï¼SRT å­—å¹•æª”å·²å„²å­˜è‡³ {srt_path}\")\n",
    "    else:\n",
    "        print(\"âš ï¸ æ²’æœ‰æ¸…ç†å¾Œçš„è½‰éŒ„å…§å®¹å¯ä¾›è½‰æ›ç‚º SRT æ ¼å¼ã€‚\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ è½‰æ›ç‚º SRT æ ¼å¼æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
