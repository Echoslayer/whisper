{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Transcript from Audio\n",
    "\n",
    "This notebook allows you to process an audio file, split it into clips, and transcribe the content using Whisper.cpp. You can configure the settings using interactive widgets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Configuration\n",
    "\n",
    "Configure the input file, output directory, and other settings using interactive widgets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "646df4a6e9ef41f28c1d7361a42afd28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='../data//demo/demo.wav', description='Input File:', layout=Layout(width='500px'), placeholder='Ent…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ca89d0be791403aa44bca0ae1152fa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='../data/output_clips', description='Output Dir:', layout=Layout(width='500px'), placeholder='Enter…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b36bd9191974c08b642533c5ec77163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=1, description='Clip Duration (min):', layout=Layout(width='500px'), max=30, min=1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "729512a79b3e4825ac7a898b9892d788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='../whisper.cpp/build/bin/whisper-cli', description='Whisper Exec:', layout=Layout(width='500px'), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10eb81eabba6419883f8c3aa9bc1471f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='../whisper.cpp/models/ggml-medium.bin', description='Whisper Model:', layout=Layout(width='500px')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "048ba1caadb4430dbd7ccda516aa299e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Language:', layout=Layout(width='500px'), options=(('Chinese (zh)', 'zh'), ('English (en…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f87b872e69074162bcdf78fe0726c26e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Transcript File:', layout=Layout(width='500px'), placeholder='Enter transcript fil…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Default configuration values\n",
    "DEFAULT_INPUT_FILE = \"../data//demo/demo.wav\"\n",
    "# DEFAULT_INPUT_FILE = \"/Volumes/MacHD/梵公子/梵公子【美女资源收集器】/第六节【识别“S”女】必听/第六节【识别“s”女】【WWW.PUACP.COM】.mp4\"\n",
    "DEFAULT_OUTPUT_DIR = \"../data/output_clips\"\n",
    "DEFAULT_CLIP_DURATION_MIN = 1  # in minutes\n",
    "DEFAULT_WHISPER_EXEC = \"../whisper.cpp/build/bin/whisper-cli\"\n",
    "DEFAULT_WHISPER_MODEL = \"../whisper.cpp/models/ggml-medium.bin\"\n",
    "DEFAULT_LANGUAGE = \"zh\"\n",
    "DEFAULT_TRANSCRIPT_FILENAME = \"transcription.txt\"\n",
    "\n",
    "# Widgets for configuration\n",
    "input_file_widget = widgets.Text(\n",
    "    value=DEFAULT_INPUT_FILE,\n",
    "    placeholder='Enter input audio file path',\n",
    "    description='Input File:',\n",
    "    layout={'width': '500px'}\n",
    ")\n",
    "\n",
    "output_dir_widget = widgets.Text(\n",
    "    value=DEFAULT_OUTPUT_DIR,\n",
    "    placeholder='Enter output directory',\n",
    "    description='Output Dir:',\n",
    "    layout={'width': '500px'}\n",
    ")\n",
    "\n",
    "clip_duration_widget = widgets.IntSlider(\n",
    "    value=DEFAULT_CLIP_DURATION_MIN,\n",
    "    min=1,\n",
    "    max=30,\n",
    "    step=1,\n",
    "    description='Clip Duration (min):',\n",
    "    layout={'width': '500px'}\n",
    ")\n",
    "\n",
    "whisper_exec_widget = widgets.Text(\n",
    "    value=DEFAULT_WHISPER_EXEC,\n",
    "    placeholder='Enter Whisper.cpp executable path',\n",
    "    description='Whisper Exec:',\n",
    "    layout={'width': '500px'}\n",
    ")\n",
    "\n",
    "whisper_model_widget = widgets.Text(\n",
    "    value=DEFAULT_WHISPER_MODEL,\n",
    "    placeholder='Enter Whisper model path',\n",
    "    description='Whisper Model:',\n",
    "    layout={'width': '500px'}\n",
    ")\n",
    "\n",
    "language_widget = widgets.Dropdown(\n",
    "    options=[('Chinese (zh)', 'zh'), ('English (en)', 'en')],\n",
    "    value=DEFAULT_LANGUAGE,\n",
    "    description='Language:',\n",
    "    layout={'width': '500px'}\n",
    ")\n",
    "\n",
    "transcript_filename_widget = widgets.Text(\n",
    "    value='',\n",
    "    placeholder=f'Enter transcript filename (default: {DEFAULT_TRANSCRIPT_FILENAME})',\n",
    "    description='Transcript File:',\n",
    "    layout={'width': '500px'}\n",
    ")\n",
    "\n",
    "# Display widgets\n",
    "display(input_file_widget)\n",
    "display(output_dir_widget)\n",
    "display(clip_duration_widget)\n",
    "display(whisper_exec_widget)\n",
    "display(whisper_model_widget)\n",
    "display(language_widget)\n",
    "display(transcript_filename_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Audio Processing Functions\n",
    "\n",
    "Import the necessary functions for processing audio files from `voice2transcripts.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the scripts directory to the path so we can import the functions\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../scripts')))\n",
    "from voice2transcripts import clear_output_folder, convert_to_wav, split_audio, transcribe_audio\n",
    "from time_stamp_cleaner import clean_transcription, save_cleaned_transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Process Audio and Generate Transcript\n",
    "\n",
    "Run the processing pipeline to convert, split, transcribe, and clean the audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗑️ 輸出資料夾已清除！\n",
      "🚀 開始音訊處理與轉錄流程...\n",
      "🔄 音訊已轉換為 WAV 格式：../data/output_clips/converted.wav\n",
      "✅ 音訊切割完成，共 2 個片段，時間戳已儲存\n",
      "🎤 轉錄片段 1/2: clip_001.wav ...\n",
      "⚠️ Whisper.cpp 訊息: error: unknown argument: --timestamps\n",
      "\n",
      "usage: ../whisper.cpp/build/bin/whisper-cli [options] file0 file1 ...\n",
      "supported audio formats: flac, mp3, ogg, wav\n",
      "\n",
      "options:\n",
      "  -h,        --help              [default] show this help message and exit\n",
      "  -t N,      --threads N         [4      ] number of threads to use during computation\n",
      "  -p N,      --processors N      [1      ] number of processors to use during computation\n",
      "  -ot N,     --offset-t N        [0      ] time offset in milliseconds\n",
      "  -on N,     --offset-n N        [0      ] segment index offset\n",
      "  -d  N,     --duration N        [0      ] duration of audio to process in milliseconds\n",
      "  -mc N,     --max-context N     [-1     ] maximum number of text context tokens to store\n",
      "  -ml N,     --max-len N         [0      ] maximum segment length in characters\n",
      "  -sow,      --split-on-word     [false  ] split on word rather than on token\n",
      "  -bo N,     --best-of N         [5      ] number of best candidates to keep\n",
      "  -bs N,     --beam-size N       [5      ] beam size for beam search\n",
      "  -ac N,     --audio-ctx N       [0      ] audio context size (0 - all)\n",
      "  -wt N,     --word-thold N      [0.01   ] word timestamp probability threshold\n",
      "  -et N,     --entropy-thold N   [2.40   ] entropy threshold for decoder fail\n",
      "  -lpt N,    --logprob-thold N   [-1.00  ] log probability threshold for decoder fail\n",
      "  -nth N,    --no-speech-thold N [0.60   ] no speech threshold\n",
      "  -tp,       --temperature N     [0.00   ] The sampling temperature, between 0 and 1\n",
      "  -tpi,      --temperature-inc N [0.20   ] The increment of temperature, between 0 and 1\n",
      "  -debug,    --debug-mode        [false  ] enable debug mode (eg. dump log_mel)\n",
      "  -tr,       --translate         [false  ] translate from source language to english\n",
      "  -di,       --diarize           [false  ] stereo audio diarization\n",
      "  -tdrz,     --tinydiarize       [false  ] enable tinydiarize (requires a tdrz model)\n",
      "  -nf,       --no-fallback       [false  ] do not use temperature fallback while decoding\n",
      "  -otxt,     --output-txt        [false  ] output result in a text file\n",
      "  -ovtt,     --output-vtt        [false  ] output result in a vtt file\n",
      "  -osrt,     --output-srt        [false  ] output result in a srt file\n",
      "  -olrc,     --output-lrc        [false  ] output result in a lrc file\n",
      "  -owts,     --output-words      [false  ] output script for generating karaoke video\n",
      "  -fp,       --font-path         [/System/Library/Fonts/Supplemental/Courier New Bold.ttf] path to a monospace font for karaoke video\n",
      "  -ocsv,     --output-csv        [false  ] output result in a CSV file\n",
      "  -oj,       --output-json       [false  ] output result in a JSON file\n",
      "  -ojf,      --output-json-full  [false  ] include more information in the JSON file\n",
      "  -of FNAME, --output-file FNAME [       ] output file path (without file extension)\n",
      "  -np,       --no-prints         [false  ] do not print anything other than the results\n",
      "  -ps,       --print-special     [false  ] print special tokens\n",
      "  -pc,       --print-colors      [false  ] print colors\n",
      "             --print-confidence  [false  ] print confidence\n",
      "  -pp,       --print-progress    [false  ] print progress\n",
      "  -nt,       --no-timestamps     [false  ] do not print timestamps\n",
      "  -l LANG,   --language LANG     [zh     ] spoken language ('auto' for auto-detect)\n",
      "  -dl,       --detect-language   [false  ] exit after automatically detecting language\n",
      "             --prompt PROMPT     [       ] initial prompt (max n_text_ctx/2 tokens)\n",
      "  -m FNAME,  --model FNAME       [../whisper.cpp/models/ggml-medium.bin] model path\n",
      "  -f FNAME,  --file FNAME        [       ] input audio file path\n",
      "  -oved D,   --ov-e-device DNAME [CPU    ] the OpenVINO device used for encode inference\n",
      "  -dtw MODEL --dtw MODEL         [       ] compute token-level timestamps\n",
      "  -ls,       --log-score         [false  ] log best decoder scores of tokens\n",
      "  -ng,       --no-gpu            [false  ] disable GPU\n",
      "  -fa,       --flash-attn        [false  ] flash attention\n",
      "  -sns,      --suppress-nst      [false  ] suppress non-speech tokens\n",
      "  --suppress-regex REGEX         [       ] regular expression matching tokens to suppress\n",
      "  --grammar GRAMMAR              [       ] GBNF grammar to guide decoding\n",
      "  --grammar-rule RULE            [       ] top-level GBNF grammar rule name\n",
      "  --grammar-penalty N            [100.0  ] scales down logits of nongrammar tokens\n",
      "\n",
      "Voice Activity Detection (VAD) options:\n",
      "             --vad                           [false  ] enable Voice Activity Detection (VAD)\n",
      "  -vm FNAME, --vad-model FNAME               [       ] VAD model path\n",
      "  -vt N,     --vad-threshold N               [0.50   ] VAD threshold for speech recognition\n",
      "  -vspd N,   --vad-min-speech-duration-ms  N [250    ] VAD min speech duration (0.0-1.0)\n",
      "  -vsd N,    --vad-min-silence-duration-ms N [100    ] VAD min silence duration (to split segments)\n",
      "  -vmsd N,   --vad-max-speech-duration-s   N [FLT_MAX] VAD max speech duration (auto-split longer)\n",
      "  -vp N,     --vad-speech-pad-ms           N [30     ] VAD speech padding (extend segments)\n",
      "  -vo N,     --vad-samples-overlap         N [0.10   ] VAD samples overlap (seconds between segments)\n",
      "✅ 片段 1/2 轉錄完成\n",
      "🎤 轉錄片段 2/2: clip_002.wav ...\n",
      "⚠️ Whisper.cpp 訊息: error: unknown argument: --timestamps\n",
      "\n",
      "usage: ../whisper.cpp/build/bin/whisper-cli [options] file0 file1 ...\n",
      "supported audio formats: flac, mp3, ogg, wav\n",
      "\n",
      "options:\n",
      "  -h,        --help              [default] show this help message and exit\n",
      "  -t N,      --threads N         [4      ] number of threads to use during computation\n",
      "  -p N,      --processors N      [1      ] number of processors to use during computation\n",
      "  -ot N,     --offset-t N        [0      ] time offset in milliseconds\n",
      "  -on N,     --offset-n N        [0      ] segment index offset\n",
      "  -d  N,     --duration N        [0      ] duration of audio to process in milliseconds\n",
      "  -mc N,     --max-context N     [-1     ] maximum number of text context tokens to store\n",
      "  -ml N,     --max-len N         [0      ] maximum segment length in characters\n",
      "  -sow,      --split-on-word     [false  ] split on word rather than on token\n",
      "  -bo N,     --best-of N         [5      ] number of best candidates to keep\n",
      "  -bs N,     --beam-size N       [5      ] beam size for beam search\n",
      "  -ac N,     --audio-ctx N       [0      ] audio context size (0 - all)\n",
      "  -wt N,     --word-thold N      [0.01   ] word timestamp probability threshold\n",
      "  -et N,     --entropy-thold N   [2.40   ] entropy threshold for decoder fail\n",
      "  -lpt N,    --logprob-thold N   [-1.00  ] log probability threshold for decoder fail\n",
      "  -nth N,    --no-speech-thold N [0.60   ] no speech threshold\n",
      "  -tp,       --temperature N     [0.00   ] The sampling temperature, between 0 and 1\n",
      "  -tpi,      --temperature-inc N [0.20   ] The increment of temperature, between 0 and 1\n",
      "  -debug,    --debug-mode        [false  ] enable debug mode (eg. dump log_mel)\n",
      "  -tr,       --translate         [false  ] translate from source language to english\n",
      "  -di,       --diarize           [false  ] stereo audio diarization\n",
      "  -tdrz,     --tinydiarize       [false  ] enable tinydiarize (requires a tdrz model)\n",
      "  -nf,       --no-fallback       [false  ] do not use temperature fallback while decoding\n",
      "  -otxt,     --output-txt        [false  ] output result in a text file\n",
      "  -ovtt,     --output-vtt        [false  ] output result in a vtt file\n",
      "  -osrt,     --output-srt        [false  ] output result in a srt file\n",
      "  -olrc,     --output-lrc        [false  ] output result in a lrc file\n",
      "  -owts,     --output-words      [false  ] output script for generating karaoke video\n",
      "  -fp,       --font-path         [/System/Library/Fonts/Supplemental/Courier New Bold.ttf] path to a monospace font for karaoke video\n",
      "  -ocsv,     --output-csv        [false  ] output result in a CSV file\n",
      "  -oj,       --output-json       [false  ] output result in a JSON file\n",
      "  -ojf,      --output-json-full  [false  ] include more information in the JSON file\n",
      "  -of FNAME, --output-file FNAME [       ] output file path (without file extension)\n",
      "  -np,       --no-prints         [false  ] do not print anything other than the results\n",
      "  -ps,       --print-special     [false  ] print special tokens\n",
      "  -pc,       --print-colors      [false  ] print colors\n",
      "             --print-confidence  [false  ] print confidence\n",
      "  -pp,       --print-progress    [false  ] print progress\n",
      "  -nt,       --no-timestamps     [false  ] do not print timestamps\n",
      "  -l LANG,   --language LANG     [zh     ] spoken language ('auto' for auto-detect)\n",
      "  -dl,       --detect-language   [false  ] exit after automatically detecting language\n",
      "             --prompt PROMPT     [       ] initial prompt (max n_text_ctx/2 tokens)\n",
      "  -m FNAME,  --model FNAME       [../whisper.cpp/models/ggml-medium.bin] model path\n",
      "  -f FNAME,  --file FNAME        [       ] input audio file path\n",
      "  -oved D,   --ov-e-device DNAME [CPU    ] the OpenVINO device used for encode inference\n",
      "  -dtw MODEL --dtw MODEL         [       ] compute token-level timestamps\n",
      "  -ls,       --log-score         [false  ] log best decoder scores of tokens\n",
      "  -ng,       --no-gpu            [false  ] disable GPU\n",
      "  -fa,       --flash-attn        [false  ] flash attention\n",
      "  -sns,      --suppress-nst      [false  ] suppress non-speech tokens\n",
      "  --suppress-regex REGEX         [       ] regular expression matching tokens to suppress\n",
      "  --grammar GRAMMAR              [       ] GBNF grammar to guide decoding\n",
      "  --grammar-rule RULE            [       ] top-level GBNF grammar rule name\n",
      "  --grammar-penalty N            [100.0  ] scales down logits of nongrammar tokens\n",
      "\n",
      "Voice Activity Detection (VAD) options:\n",
      "             --vad                           [false  ] enable Voice Activity Detection (VAD)\n",
      "  -vm FNAME, --vad-model FNAME               [       ] VAD model path\n",
      "  -vt N,     --vad-threshold N               [0.50   ] VAD threshold for speech recognition\n",
      "  -vspd N,   --vad-min-speech-duration-ms  N [250    ] VAD min speech duration (0.0-1.0)\n",
      "  -vsd N,    --vad-min-silence-duration-ms N [100    ] VAD min silence duration (to split segments)\n",
      "  -vmsd N,   --vad-max-speech-duration-s   N [FLT_MAX] VAD max speech duration (auto-split longer)\n",
      "  -vp N,     --vad-speech-pad-ms           N [30     ] VAD speech padding (extend segments)\n",
      "  -vo N,     --vad-samples-overlap         N [0.10   ] VAD samples overlap (seconds between segments)\n",
      "✅ 片段 2/2 轉錄完成\n",
      "🎉 轉錄處理完成！轉錄結果已儲存至 ../data/output_clips/../transcripts/transcription.txt\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Get values from widgets\n",
    "    input_file = input_file_widget.value\n",
    "    output_dir = output_dir_widget.value\n",
    "    clip_duration_sec = clip_duration_widget.value * 60  # Convert minutes to seconds\n",
    "    whisper_exec = whisper_exec_widget.value\n",
    "    whisper_model = whisper_model_widget.value\n",
    "    language = language_widget.value\n",
    "    transcript_filename = transcript_filename_widget.value if transcript_filename_widget.value.strip() else DEFAULT_TRANSCRIPT_FILENAME\n",
    "\n",
    "    # Check if input file exists\n",
    "    if not os.path.exists(input_file):\n",
    "        raise FileNotFoundError(f\"❌ 找不到輸入音訊檔案：{input_file}\")\n",
    "\n",
    "    # Clear old files\n",
    "    clear_output_folder(output_dir)\n",
    "\n",
    "    # Execute the processing pipeline\n",
    "    print(\"🚀 開始音訊處理與轉錄流程...\")\n",
    "    wav_file = convert_to_wav(input_file, output_dir)\n",
    "    clip_files = split_audio(wav_file, clip_duration_sec, output_dir)\n",
    "    transcribe_audio(clip_files, output_dir, whisper_exec, whisper_model, language, transcript_filename)\n",
    "    print(f\"🎉 轉錄處理完成！轉錄結果已儲存至 {os.path.join(output_dir, '../transcripts/' + transcript_filename)}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ 處理過程中發生錯誤：{e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Clean Transcription\n",
    "\n",
    "Clean the transcription by removing per-sentence timestamps and formatting the content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.1: Configure Cleaned Transcript Filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d0302d336f042cc847a0bc6ed2252a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Cleaned Transcript:', layout=Layout(width='500px'), placeholder='Enter cleaned tra…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Widget for cleaned transcript filename\n",
    "cleaned_transcript_filename_widget = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Enter cleaned transcript filename (default: clean_transcription.txt)',\n",
    "    description='Cleaned Transcript:',\n",
    "    layout={'width': '500px'}\n",
    ")\n",
    "display(cleaned_transcript_filename_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.2: Clean and Save Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 開始清理轉錄結果...\n",
      "嘗試讀取轉錄檔案：../data/transcripts/transcription.txt\n",
      "✅ 找到轉錄檔案，開始清理...\n",
      "⚠️ 沒有找到有效的轉錄內容，無法儲存清理後的檔案。\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Get the transcript file path based on the output directory structure\n",
    "    transcript_dir = os.path.join(os.path.dirname(output_dir), 'transcripts')\n",
    "    transcript_path = os.path.join(transcript_dir, transcript_filename)\n",
    "    \n",
    "    # Get the cleaned transcript filename from the widget\n",
    "    cleaned_transcript_filename = cleaned_transcript_filename_widget.value if cleaned_transcript_filename_widget.value.strip() else \"clean_transcription.txt\"\n",
    "    cleaned_transcript_path = os.path.join(transcript_dir, cleaned_transcript_filename)\n",
    "    \n",
    "    # Read, clean, and save the transcription\n",
    "    print(\"🧹 開始清理轉錄結果...\")\n",
    "    print(f\"嘗試讀取轉錄檔案：{transcript_path}\")\n",
    "    if os.path.exists(transcript_path):\n",
    "        print(f\"✅ 找到轉錄檔案，開始清理...\")\n",
    "        with open(transcript_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        global cleaned_segments\n",
    "        cleaned_segments = clean_transcription(text)\n",
    "        if cleaned_segments:\n",
    "            save_cleaned_transcription(cleaned_segments, cleaned_transcript_path)\n",
    "            print(f\"🎉 清理完成！清理後的轉錄結果已儲存至 {cleaned_transcript_path}\")\n",
    "        else:\n",
    "            print(\"⚠️ 沒有找到有效的轉錄內容，無法儲存清理後的檔案。\")\n",
    "    else:\n",
    "        print(f\"❌ 轉錄檔案不存在：{transcript_path}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"❌ 找不到檔案：{e}\")\n",
    "except IOError as e:\n",
    "    print(f\"❌ 讀取檔案時發生錯誤：{e}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ 清理轉錄過程中發生未知錯誤：{e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Convert Transcription to SRT Format\n",
    "\n",
    "Convert the cleaned transcription to SRT subtitle format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.1: Configure SRT Filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f759a214c9e7423482e8233f6970391f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='subtitles.srt', description='SRT File:', layout=Layout(width='500px'), placeholder='Enter SRT file…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Widget for SRT filename\n",
    "srt_filename_widget = widgets.Text(\n",
    "    value='subtitles.srt',\n",
    "    placeholder='Enter SRT filename (default: subtitles.srt)',\n",
    "    description='SRT File:',\n",
    "    layout={'width': '500px'}\n",
    ")\n",
    "display(srt_filename_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.2: Generate SRT File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 開始轉換為 SRT 格式...\n",
      "⚠️ 沒有清理後的轉錄內容可供轉換為 SRT 格式。\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from time_stamp_cleaner import convert_to_srt\n",
    "    \n",
    "    # Get the SRT filename from the widget\n",
    "    srt_filename = srt_filename_widget.value if srt_filename_widget.value.strip() else \"subtitles.srt\"\n",
    "    srt_path = os.path.join(transcript_dir, srt_filename)\n",
    "    \n",
    "    # Convert cleaned segments to SRT format\n",
    "    print(\"📝 開始轉換為 SRT 格式...\")\n",
    "    if cleaned_segments:\n",
    "        convert_to_srt(cleaned_segments, srt_path)\n",
    "        print(f\"🎉 轉換完成！SRT 字幕檔已儲存至 {srt_path}\")\n",
    "    else:\n",
    "        print(\"⚠️ 沒有清理後的轉錄內容可供轉換為 SRT 格式。\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ 轉換為 SRT 格式時發生錯誤：{e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
